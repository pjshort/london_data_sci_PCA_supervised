{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries\n",
    "First we need to load the required Python libraries. Libraries are like extensions to the base python that add functionality or help to carry out specific tasks. \n",
    "\n",
    "We will load some libraries that will boost your data handling capacity. The main ones are numpy and pandas - we will call them `np` and `pd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the heart attack data set\n",
    "\n",
    "Use `pd.read_csv` to read in the file (add header=None).\n",
    "\n",
    "Use `pd.shape` to check dimensions and `pd.head()` to take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heart = pd.read_csv(\"./processed.cleveland.data.clean\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heart.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the heart attack data set\n",
    "\n",
    "What are we looking at?\n",
    "\n",
    "The last column is different kinds of heart attacks (0 is none, 1,2,3,4 are different grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heart.columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our goal: predict heart disease from the data\n",
    "\n",
    "For now, we will treat the last column 'num' as a binary outcome. 0 is no heart disease, anything higher than zero is heart disease. We can do multivariate classification later!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = heart.iloc[:,0:13]\n",
    "y = heart.iloc[:,-1]\n",
    "y[y > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take out the last row and call it `num` and keep all the other rows in a separate data.frame called `attributes`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal components analysis can reveal clustering\n",
    "\n",
    "Use scikit-learn `StandardScaler()` to scale all of the features and use `seaborn` to produce a pairplot and a heatmap of the correlctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "X_scaled = pd.DataFrame(ss.fit_transform(X))\n",
    "X_scaled.columns = X.columns\n",
    "X = X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corrmat = X.corr()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(corrmat, \n",
    "            linewidths=0.5, \n",
    "            cmap=\"RdBu\", \n",
    "            vmin=-1, \n",
    "            vmax=1, \n",
    "            annot=True)\n",
    "\n",
    "plt.xticks(rotation=270);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use scikit-learn `PCA` to calculate the PCs and display the variance explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "sns.barplot(x=np.arange(0,pca.n_components_)+1, y=pca.explained_variance_ratio_)\n",
    "plt.xticks(np.arange(0, pca.n_components_), np.arange(0, pca.n_components_))\n",
    "\n",
    "plt.figure()\n",
    "sns.barplot(np.arange(pca.n_components_)+1, np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xticks(np.arange(0, pca.n_components_), np.arange(0, pca.n_components_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Check out the `components` of the PCA object - what do they mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "components = pd.DataFrame(pca.components_, \n",
    "                        columns = X.columns,\n",
    "                        index   = X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: display the components as a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(\n",
    "        components.transpose(), \n",
    "        linewidths=0.5, \n",
    "        cmap=\"RdBu\", \n",
    "        vmin=-1, vmax=1, annot=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the first two principal components and overlay the disease state as the color to see if they cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_pca = pd.DataFrame(pca.transform(X))\n",
    "X_pca.columns = ['PC' + str(i) for i in range(0,13)]\n",
    "X_pca = X_pca.join(y)\n",
    "X_pca.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='PC1', y='PC2', data=X_pca, fit_reg=False, hue = 'num')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus (if you are ahead):\n",
    "\n",
    "Load the iris data set using  and do PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "target = pd.DataFrame(iris.target)\n",
    "iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "iris_pca = pd.DataFrame(pca.fit_transform(iris))\n",
    "iris_pca.columns = ['PC' + str(i) for i in range(0,4)]\n",
    "#iris_pca = iris_pca.join(target)\n",
    "\n",
    "iris_pca['target'] = target\n",
    "iris_pca.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='PC1', y='PC2', data=iris_pca, fit_reg=False, hue = \"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple classifier using Logistic Regression\n",
    "\n",
    "A logistic regression models a binary outcome using a linear model, which is transformed into a 'sigma' shape.\n",
    "\n",
    "This is a very commonly used model and a great place to start. \n",
    "\n",
    "First, use `train_test_split` to 'hide' some of our data to use for testing later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print (\"XTrain dimensions: \", X_train.shape)\n",
    "print (\"yTrain dimensions: \", y_train.shape)\n",
    "print (\"XTest dimensions: \", X_train.shape)\n",
    "print (\"yTest dimensions: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "log_reg = LogisticRegression(class_weight='balanced')\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "print (classification_report(y_test, y_pred))\n",
    "#print (\"Overall Accuracy:\", round(metrics.accuracy_score(y_test, y_pred),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving on to Random Forest\n",
    "\n",
    "We will use a `RandomForestClassifier` to try to build a model to distinguish heart disease from healthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can initialize the RandomForestClassifier and fit a model to our training data. Use `n_estimators=100` to use 100 different trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=2, random_state=0, n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `X_test` as an input to the classifer to get out `y_pred`. We can use the `classification_report` and `confusion_matrix` from scikit-learn to get an idea of how well we are performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (classification_report(y_test, y_pred))\n",
    "print (\"Overall Accuracy: \", round(accuracy_score(y_test, y_pred), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = confusion_matrix(y_test, y_pred) \n",
    "print (mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, but we only ran 100 trees... if we create more than 100 trees, do we get better accuracy? Where does it stop being worth making more trees?\n",
    "\n",
    "Create a plot for different values from 100 to 1000 trees. This may take a while!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Range of `n_estimators` values to explore.\n",
    "min_estimators = 100\n",
    "max_estimators = 1000\n",
    "\n",
    "error_rate = []\n",
    "\n",
    "for i in range(min_estimators, max_estimators + 1, 50):\n",
    "    clf.set_params(n_estimators=i)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    error_rate.append( 1 - accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\n",
    "plt.plot(range(min_estimators, max_estimators + 1, 50), error_rate)\n",
    "\n",
    "plt.xlim(min_estimators, max_estimators)\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"OOB error rate\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RandomForestClassifier` calculates a feature importance for each of the inputs. We can visualize these using seaborn barplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importance = clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.barplot(x = importance, y = X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Bonus) Make a 'receiver operating characteristic' (ROC) curve and compare logistic regression and random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.set_params(n_estimators = 10000)\n",
    "clf.fit(X_train, y_train)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = clf.predict_proba(X_test)[:, 1]\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)\n",
    "\n",
    "y_pred_log_reg = log_reg.predict_proba(X_test)[:, 1]\n",
    "fpr_log_reg, tpr_log_reg, _ = roc_curve(y_test, y_pred_log_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
    "plt.plot(fpr_log_reg, tpr_log_reg, label='LogReg')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Double Bonus) Multi-label classification\n",
    "\n",
    "At the beginning of the notebook, we took all of the different grades of heart disease (1,2,3,4) and made them all 1 to represent any type of heart disease.\n",
    "\n",
    "Use RandomForest to do multi-label classification - how does it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heart = pd.read_csv(\"./processed.cleveland.data.clean\", header=None)\n",
    "X = heart.iloc[:,0:13]\n",
    "y = heart.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=2, random_state=0, n_estimators=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print (classification_report(y_test, y_pred))\n",
    "print (\"Overall Accuracy: \", round(accuracy_score(y_test, y_pred), 2))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Recap\n",
    "\n",
    "There are _dozens_ of different supervised learning techniques, all with different strengths, weaknesses, and applications.\n",
    "\n",
    "- We learned two, logistic regression and random forests.\n",
    "- These models are relatively simple to understand, while still providing great performance. Random Forests provide the additional benefit of being able to model non-linear relationships.\n",
    "- More important than the models themselves are the principles of training and testing, and model evaluation. These will not change nearly as much as the underlying models!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
